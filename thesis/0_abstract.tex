Social media discussions on climate change leverage multimodal content (text, images) to convey complex emotions, yet emotion classification faces challenges from scarce labelled data and nuanced text-image interactions. This thesis employs state-of-the-art text models (e.g., RoBERTa, BART) and CLIP-based vision encoders in zero-shot and weakly supervised frameworks to infer emotional responses triggered in people by climate-related posts. Using the ClimateTV dataset, weakly supervised techniques—self-training, loss re-weighting, and fine-tuning with soft labels—are applied. Results show that text models pre-trained on tweets (CardiffNLP’s RoBERTa) outperform others, CLIP-based models improve with fine-tuning, and multimodal fusion surpasses single-modality approaches. However, consolidating labels into basic emotions risks "flattening" distributions, overlooking minority classes and causing misclassifications. This highlights tensions between aggregate performance and instance-level accuracy, necessitating refined label aggregation, advanced metrics, and targeted loss functions to mitigate biases. This work advances methodological approaches for analyzing public emotional responses to climate discourse while highlighting key gaps for future research.