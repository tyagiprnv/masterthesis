Social media discussions on climate change often combine text and images to express complex emotions, yet accurately classifying these emotions remains challenging due to limited labeled data and intricate text-image interactions. This thesis explores multimodal emotion classification using the ClimateTV dataset, leveraging state-of-the-art text models (e.g., RoBERTa, BART) and CLIP-based vision encoders to infer emotional responses to climate-related posts. Results indicate that text models pre-trained on tweets (e.g., CardiffNLPâ€™s RoBERTa) outperform general-purpose models, while fine-tuning CLIP-based models enhances performance. Furthermore, multimodal fusion consistently surpasses single-modality approaches. However, aggregating labels into broad emotion categories risks misclassifying minority emotions, revealing a trade-off between overall accuracy and instance-level precision. Addressing these limitations requires refined label aggregation strategies, improved evaluation metrics, and specialized loss functions to mitigate biases. This work advances methodological approaches for analyzing emotional responses to climate discourse while identifying key areas for future research in multimodal emotion classification.