In this chapter, we delve into the challenges and outcomes of emotion classification to social media data—specifically climate-related posts where emotional context can be nuanced and diverse. We begin by examining the baseline performance of various large language models, both domain-specific (CardiffNLP RoBERTa models) and general-purpose (e.g., DeBERTa-v3-large), highlighting how domain adaptation can yield measurable improvements in Exact Match and Top-3 Accuracy.

Next, we discuss the motivation for consolidating eleven initially predicted labels into Ekman’s six basic emotions (anger, fear, surprise, disgust, sadness, joy). This step enhances interpretability and consistency in a space often plagued by class imbalance and label ambiguity. We then present detailed experimental results showing that while distribution-level improvements (e.g., higher cosine similarity) are attainable through label aggregation and soft-label averaging, these gains do not necessarily translate to accurate per-instance predictions. Confusion matrices and Mean Squared Error (MSE) analyses reveal systemic biases across text-only, image-only, and multimodal models. Finally, we propose future directions—ranging from hybrid loss functions and dynamic fusion mechanisms to refined label aggregation methods—that aim to mitigate minority label collapse and address the tension between distribution alignment and real-world applicability.

By considering these findings and open challenges, this chapter sets the stage for improving zero-shot and minimally supervised approaches in emotion classification. The goal is to inform both researchers and practitioners about the multifaceted nature of model performance, ensuring that future efforts balance distribution-level fidelity with fine-grained predictive accuracy.

\section{Analysis of Findings}
\subsection{Zero-shot performance}

The results align with expectations, as the CardiffNLP RoBERTa models, pre-trained on Twitter data, outperformed general-purpose models in emotion classification. Their higher Exact Match and Top-3 Accuracy confirm the advantage of domain-specific pre-training for social media text.
\newline

Applying a confidence threshold of 0.9 improved precision but significantly reduced the number of retained predictions. The DeBERTa-v3-large-Zeroshot model had the highest precision (72.73\%) but generated fewer confident predictions, indicating greater uncertainty. In contrast, CardiffNLP models retained the most high-confidence predictions (~63\%), demonstrating both reliability and coverage.
\newline

These findings reinforce that while zero-shot models benefit from domain adaptation, moderate Exact Match scores (~65\%) suggest they still struggle with emotional nuances. Moreover, the best zero-shot model i.e. CardiffNLP RoBERTa-Large, predicted over 11 different labels, leading to inconsistencies in label distribution and interpretation. To address this, we consolidated the predictions into a more structured and theoretically grounded framework.


\subsection{Experimental Results: Distribution vs. Instance-Level Accuracy}

\subsubsection*{Label Aggregation and Distribution Flattening}
The manual mapping of emotions (e.g., merging \textit{love}, \textit{optimism}, and \textit{trust} into \emph{joy}) introduced semantic ambiguity, as evidenced by the confusion matrices (Figures \ref{fig:text-confusion }--\ref{fig:multi-confusion }). For instance:
\newline

\textbf{Overprediction of Merged Classes}: \emph{joy} and \emph{surprise} (which absorbed multiple original labels) dominated predictions. For our best text-only model, \emph{joy} had 162 correct predictions but 149 false positives (e.g., \emph{anger}/\emph{sadness} misclassified as \emph{joy}). Qualitative examples highlight this confusion: a tweet about climate strikes (Case B) elicited replies dominated by \emph{surprise} (62.4\%), but the model redistributed probabilities to \emph{fear} and \emph{anger} (Figure \ref{fig:bad_cases}), conflating semantically distinct emotions. Similarly, \emph{surprise} (merged with anticipation) was overpredicted for \emph{anger} (22/44 instances) and \emph{sadness} (43/83).
\newline

\textbf{Minority Class Collapse}: Rare emotions like \emph{fear} and \emph{sadness} suffered catastrophic recall ($<10\%$ in our best multimodal model), with \emph{fear} predictions often collapsing into \emph{surprise} or \emph{joy}. For example, in Case A (Figure \ref{fig:bad_cases}), a reply explicitly describing wildfires in sub-Saharan Africa as “fires broke out in entire countries” was assigned only 27\% probability to \emph{fear}—half the ground truth value (48.8\%)—with excess mass allocated to \emph{surprise} and \emph{anger}. The MSE values for \emph{sadness} (0.0069–0.0162) and \emph{fear} (0.0120–0.0293) were consistently low, suggesting the model learned to minimize their contributions to the loss by predicting near-zero probabilities.
\newline

The fine-tuning setup with KL divergence loss exacerbated this by prioritizing smooth, distribution-wide fidelity over per-class accuracy. Case B (Figure \ref{fig:bad_cases}) exemplifies this: the model’s prediction for a reply expressing 62.4\% \emph{surprise} was flattened to 46.5\%, with the remaining probability spread across unrelated classes like \emph{fear} and \emph{anger}. Averaging labels across replies further flattened the target distributions, incentivizing models to avoid confident predictions for minority classes.

\subsubsection*{Modality-Specific Biases and Error Propagation}
The MSE tables highlight modality-specific weaknesses:
\newline

\textbf{Text Models}: Achieved the lowest \emph{anger} MSE (0.0169 vs. baseline 0.0224) but failed to classify \emph{anger} instances (2/44 correct in our best text-only model). Case D illustrates this: while textual cues like “This is stupid” (expressing anger toward political decisions) were present, the text model conflated \emph{anger} with \emph{disgust} due to overlapping lexicon.
\newline

\textbf{Image Models}: Overpredicted \emph{surprise} (361 false positives), likely due to CLIP’s bias toward visually salient cues. In Case C (Figure \ref{fig:good_cases}), the Arctic ice melt graph image drove surprise overprediction (46.5\%) despite replies emphasizing \emph{fear}—a disconnect between visual salience and textual sentiment. 
\newline

\textbf{Multimodal Fusion}: While our best multimodal model reduced \emph{anger} MSE (0.0120 vs. baseline 0.0278), confusion matrices reveal it inherited text’s overprediction of \emph{surprise}. Case C demonstrates this: the fusion model assigned 27.7\% probability to \emph{surprise} (matching the aggregated replies) but misclassified \emph{fear} (14.8\% vs. 27.7\% ground truth), reflecting unresolved cross-modal conflicts.

\subsubsection*{Metric Misalignment and Practical Implications}
The discrepancy between cosine similarity (aggregate distribution alignment) and confusion matrices (instance-level errors) raises questions about metric suitability:
\newline

\textbf{Cosine Similarity} rewards global shape matching but masks critical misclassifications. Case C exemplifies this: high cosine similarity was achieved by matching the spread of \emph{joy}, \emph{surprise}, and \emph{fear}, but the model failed to detect the true intensity of \emph{fear} (14.8\% vs. 27.7\%).
\newline

\textbf{MSE} penalizes large deviations but is insensitive to class swaps. In Case D, the model’s confusion between \emph{anger} and \emph{disgust} (both high-probability classes) resulted in low MSE despite misclassifications critical for tracking public sentiment.

\subsubsection*{Overfitting and Generalization Trends}
The observed train and validation loss curves further confirm the overfitting tendency of tuned models as shown in Tables (\ref{tab:hparam-breakdown}--\ref{tab:multi-hparam-breakdown}). Typically, when models are tuned, the training loss decreases to very low values, whereas frozen models exhibit a much higher loss that decreases more slowly. The validation loss curve shows the opposite effect, reinforcing the hypothesis that tuning leads to overfitting. For a detailed visualization of these trends, refer to the loss curve images in the Appendix chapter \ref{app:losses}.
\newline

For real-world applications (e.g., tracking \emph{anger} in crisis responses), these metrics poorly reflect operational needs. A model with high cosine similarity could still fail to detect critical emotions.


\section{Future Directions}
To address the identified limitations, we propose six research directions combining methodological innovation with rigorous evaluation:

\begin{itemize}
    \item \textbf{Hybrid and Weighted Loss Functions}
    \begin{itemize}
        \item \textbf{Why}: Purely distribution-focused objectives (e.g., KL divergence) risk "flattening" predictions and neglecting minority classes.
        \item \textbf{How}: Combine these objectives with focal or class-weighted losses to emphasize labels like \textit{sadness} or \textit{fear}, which may be underrepresented even in zero-shot outputs.
        \begin{equation}
            \mathcal{L} = \alpha D_{KL}(P \parallel Q) + (1-\alpha)\mathcal{L}_{\text{Class-weighted}}
        \end{equation}
    \end{itemize}
    
    \item \textbf{Refined Label Generation and Aggregation}
    \begin{itemize}
        \item \textbf{Why}: Relying solely on zero-shot models for soft labels can propagate upstream biases and simple aggregation strategies like averaging can magnify them (e.g., merging \textit{anticipation} and \textit{surprise}).
        \item \textbf{How}: Consider more nuanced label aggregation (e.g., attention-weighted merging) to preserve emotional specificity, or incorporate partial human labeling to calibrate and refine zero-shot predictions.
    \end{itemize}
    
    \item \textbf{Multi-Task and Disentangled Training}
    \begin{itemize}
        \item \textbf{Why}: A single objective often cannot balance overall distribution alignment with per-class accuracy, especially when label quality is uncertain.
        \item \textbf{How}: First optimize on the full, zero-shot-labeled dataset for broad coverage, then fine-tune on smaller, more carefully curated or partially human-annotated subsets to correct for minority-class underrepresentation.
    \end{itemize}
    
    \item \textbf{Dynamic Fusion and Modality Gating}
    \begin{itemize}
        \item \textbf{Why}: Text and image modalities provide complementary signals, but the confidence of zero-shot models may vary across modalities.
        \item \textbf{How}: Implement gating mechanisms or attention-based weighting to override misleading cues in one modality when the other provides stronger evidence.
        \begin{equation}
            w_{\text{text}}, w_{\text{image}} = f_{\text{attention}}(s_{\text{text}}, s_{\text{image}})
        \end{equation}
    \end{itemize}
    
    \item \textbf{Enhanced Evaluation Protocols}
    \begin{itemize}
        \item \textbf{Why}: Metrics that only measure distribution alignment can mask performance on minority classes and overestimate real-world suitability.
        \item \textbf{How}: Pair distribution-based metrics (KL divergence, Earth Mover’s Distance) with per-class indicators (macro-F1) to surface critical performance gaps.
    \end{itemize}
    
    \item \textbf{Metadata and Bias Mitigation}
    \begin{itemize}
        \item \textbf{Why}: Zero-shot models may reflect cultural or demographic biases in their pretrained distributions.
        \item \textbf{How}: Incorporate metadata (e.g., annotator demographics or domain context) when available, to model and mitigate hidden biases within soft-label predictions.
    \end{itemize}
\end{itemize}

By coupling zero-shot label generation with carefully chosen loss functions, gating strategies, and evaluation frameworks, future research can reduce the risk of "flattened" distributions and minority label collapse. This holistic approach is pivotal for modelling real-world emotional complexity, ensuring that both distribution-level and instance-level performance remains robust—despite the challenges of working with fully unlabelled datasets



