This thesis set out to explore how state-of-the-art text and image models can enhance our understanding of the emotional impact of climate-related content on social media, particularly in the absence of explicit emotion labels. Our investigation demonstrated that domain-specific pre-training, such as CardiffNLP RoBERTa, provides a notable advantage over general-purpose language models for analyzing emotionally charged social media text.
\newline

Beyond text-based insights, our findings underscore the potential of multimodal fusion: integrating textual and visual features consistently improved emotion classification performance compared to unimodal approaches. However, this fusion also introduced new challenges, such as an overestimation of emotions like surprise when processing visually striking images. These challenges highlight the need for careful model calibration to balance distribution-wide fidelity with per-instance accuracy, particularly when working with inherently noisy social media data.
\newline

To address these limitations, we proposed various strategies, including hybrid loss functions, refined label aggregation techniques, and adaptive fusion mechanisms. By refining these approaches, future research can develop more robust and context-aware emotion classification systems. Such advancements will empower researchers, policymakers, and activists to better understand and respond to public sentiment surrounding climate change, ultimately fostering more effective communication and engagement strategies.