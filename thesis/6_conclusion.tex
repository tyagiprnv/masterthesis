This thesis set out to determine how state-of-the-art text and image models can be leveraged—through zero-shot and weakly supervised methods—to reveal the emotional dynamics of climate change discourse on social media, where explicit labels are scarce. We began by illustrating how domain-specific pre-training (e.g., CardiffNLP RoBERTa) confers an advantage over general-purpose language models, particularly for emotionally charged social media text. We also demonstrated that consolidating fine-grained zero-shot outputs into Ekman’s six basic emotions improves aggregate distribution metrics but can overlook nuances and lead to “collapse” for minority emotions such as fear and sadness. 
\newline

In parallel, our findings highlight the value of multimodal fusion: combining text and images consistently enhanced performance over unimodal approaches, yet also introduced new challenges, such as overpredicting surprise when confronted with striking visuals. These issues underscore the importance of carefully balancing distribution-wide fidelity against per-instance accuracy—especially when working with noisy social media data. Finally, we proposed several avenues for improvement, including hybrid loss functions, refined label-aggregation strategies, and adaptive fusion mechanisms. By addressing these challenges, future work can deliver more accurate, context-sensitive emotion classification systems that help researchers, policymakers, and activists better understand and respond to the public’s emotional engagement with climate change.